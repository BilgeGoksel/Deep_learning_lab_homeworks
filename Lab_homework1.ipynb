{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/HP/Desktop/DERSLER/Derin Öğrenme Lab/lab3_12032025/BankNote_Authentication.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1372 entries, 0 to 1371\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   variance  1372 non-null   float64\n",
      " 1   skewness  1372 non-null   float64\n",
      " 2   curtosis  1372 non-null   float64\n",
      " 3   entropy   1372 non-null   float64\n",
      " 4   class     1372 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 53.7 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.36840</td>\n",
       "      <td>9.6718</td>\n",
       "      <td>-3.9606</td>\n",
       "      <td>-3.16250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance  skewness  curtosis  entropy  class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0\n",
       "5   4.36840    9.6718   -3.9606 -3.16250      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance    0\n",
       "skewness    0\n",
       "curtosis    0\n",
       "entropy     0\n",
       "class       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi eğitim ve test kümelerine ayırma\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Veri normalizasyonu\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_layers, output_size=1, learning_rate=0.1, activation=\"relu\"):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation_function = activation\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        layer_sizes = [input_size] + hidden_layers + [output_size]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * 0.1)\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
    "\n",
    "    def activation(self, x):\n",
    "        if self.activation_function == \"tanh\":\n",
    "            return np.tanh(x)\n",
    "        elif self.activation_function == \"relu\":\n",
    "            return np.maximum(0, x)\n",
    "\n",
    "    def activation_derivative(self, x):\n",
    "        if self.activation_function == \"tanh\":\n",
    "            return 1 - np.tanh(x) ** 2\n",
    "        elif self.activation_function == \"relu\":\n",
    "            return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        activations = [X]\n",
    "        inputs = []\n",
    "\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            inputs.append(z)\n",
    "            activations.append(self.activation(z))\n",
    "\n",
    "        z_output = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        inputs.append(z_output)\n",
    "        activations.append(1 / (1 + np.exp(-z_output)))\n",
    "\n",
    "        return activations, inputs\n",
    "\n",
    "    def compute_cost(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def back_propagation(self, activations, inputs, y_true):\n",
    "        m = y_true.shape[0]\n",
    "        grads_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grads_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        dz = activations[-1] - y_true.reshape(-1, 1)\n",
    "        grads_w[-1] = np.dot(activations[-2].T, dz) / m\n",
    "        grads_b[-1] = np.sum(dz, axis=0, keepdims=True) / m\n",
    "\n",
    "        for i in range(len(self.hidden_layers) - 1, -1, -1):\n",
    "            dz = np.dot(dz, self.weights[i + 1].T) * self.activation_derivative(inputs[i])\n",
    "            grads_w[i] = np.dot(activations[i].T, dz) / m\n",
    "            grads_b[i] = np.sum(dz, axis=0, keepdims=True) / m\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * grads_w[i]\n",
    "            self.biases[i] -= self.learning_rate * grads_b[i]\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=200):\n",
    "        for epoch in range(epochs):\n",
    "            activations, inputs = self.forward_propagation(X_train)\n",
    "            cost = self.compute_cost(y_train, activations[-1])\n",
    "            self.back_propagation(activations, inputs, y_train)\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}: Cost = {cost:.4f}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward_propagation(X)\n",
    "        return (activations[-1] >= 0.5).astype(int)\n",
    "\n",
    "models = {\n",
    "    \"tanh_1\": MLP(input_size=X.shape[1], hidden_layers=[5], activation=\"tanh\"),\n",
    "    \"tanh_2\": MLP(input_size=X.shape[1], hidden_layers=[5, 5], activation=\"tanh\"),\n",
    "    \"relu_1\": MLP(input_size=X.shape[1], hidden_layers=[5], activation=\"relu\"),\n",
    "    \"relu_2\": MLP(input_size=X.shape[1], hidden_layers=[5, 5], activation=\"relu\")\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP with TANH activation and 1 hidden layer(s)...\n",
      "Epoch 10: Cost = 0.6615\n",
      "Epoch 20: Cost = 0.6343\n",
      "Epoch 30: Cost = 0.5991\n",
      "Epoch 40: Cost = 0.5573\n",
      "Epoch 50: Cost = 0.5118\n",
      "Epoch 60: Cost = 0.4657\n",
      "Epoch 70: Cost = 0.4207\n",
      "Epoch 80: Cost = 0.3777\n",
      "Epoch 90: Cost = 0.3370\n",
      "Epoch 100: Cost = 0.2994\n",
      "Epoch 110: Cost = 0.2652\n",
      "Epoch 120: Cost = 0.2350\n",
      "Epoch 130: Cost = 0.2090\n",
      "Epoch 140: Cost = 0.1869\n",
      "Epoch 150: Cost = 0.1683\n",
      "Epoch 160: Cost = 0.1528\n",
      "Epoch 170: Cost = 0.1399\n",
      "Epoch 180: Cost = 0.1291\n",
      "Epoch 190: Cost = 0.1199\n",
      "Epoch 200: Cost = 0.1121\n",
      "Epoch 210: Cost = 0.1054\n",
      "Epoch 220: Cost = 0.0996\n",
      "Epoch 230: Cost = 0.0946\n",
      "Epoch 240: Cost = 0.0902\n",
      "Epoch 250: Cost = 0.0863\n",
      "Epoch 260: Cost = 0.0829\n",
      "Epoch 270: Cost = 0.0798\n",
      "Epoch 280: Cost = 0.0770\n",
      "Epoch 290: Cost = 0.0745\n",
      "Epoch 300: Cost = 0.0722\n",
      "Epoch 310: Cost = 0.0702\n",
      "Epoch 320: Cost = 0.0683\n",
      "Epoch 330: Cost = 0.0665\n",
      "Epoch 340: Cost = 0.0649\n",
      "Epoch 350: Cost = 0.0635\n",
      "Epoch 360: Cost = 0.0621\n",
      "Epoch 370: Cost = 0.0608\n",
      "Epoch 380: Cost = 0.0596\n",
      "Epoch 390: Cost = 0.0585\n",
      "Epoch 400: Cost = 0.0574\n",
      "Training MLP with TANH activation and 2 hidden layer(s)...\n",
      "Epoch 10: Cost = 0.6902\n",
      "Epoch 20: Cost = 0.6884\n",
      "Epoch 30: Cost = 0.6871\n",
      "Epoch 40: Cost = 0.6861\n",
      "Epoch 50: Cost = 0.6852\n",
      "Epoch 60: Cost = 0.6842\n",
      "Epoch 70: Cost = 0.6830\n",
      "Epoch 80: Cost = 0.6813\n",
      "Epoch 90: Cost = 0.6789\n",
      "Epoch 100: Cost = 0.6753\n",
      "Epoch 110: Cost = 0.6695\n",
      "Epoch 120: Cost = 0.6600\n",
      "Epoch 130: Cost = 0.6443\n",
      "Epoch 140: Cost = 0.6181\n",
      "Epoch 150: Cost = 0.5763\n",
      "Epoch 160: Cost = 0.5168\n",
      "Epoch 170: Cost = 0.4450\n",
      "Epoch 180: Cost = 0.3717\n",
      "Epoch 190: Cost = 0.3043\n",
      "Epoch 200: Cost = 0.2465\n",
      "Epoch 210: Cost = 0.1998\n",
      "Epoch 220: Cost = 0.1641\n",
      "Epoch 230: Cost = 0.1377\n",
      "Epoch 240: Cost = 0.1183\n",
      "Epoch 250: Cost = 0.1039\n",
      "Epoch 260: Cost = 0.0930\n",
      "Epoch 270: Cost = 0.0845\n",
      "Epoch 280: Cost = 0.0778\n",
      "Epoch 290: Cost = 0.0724\n",
      "Epoch 300: Cost = 0.0680\n",
      "Epoch 310: Cost = 0.0642\n",
      "Epoch 320: Cost = 0.0610\n",
      "Epoch 330: Cost = 0.0583\n",
      "Epoch 340: Cost = 0.0558\n",
      "Epoch 350: Cost = 0.0537\n",
      "Epoch 360: Cost = 0.0518\n",
      "Epoch 370: Cost = 0.0501\n",
      "Epoch 380: Cost = 0.0485\n",
      "Epoch 390: Cost = 0.0471\n",
      "Epoch 400: Cost = 0.0458\n",
      "Training MLP with RELU activation and 1 hidden layer(s)...\n",
      "Epoch 10: Cost = 0.6859\n",
      "Epoch 20: Cost = 0.6793\n",
      "Epoch 30: Cost = 0.6712\n",
      "Epoch 40: Cost = 0.6576\n",
      "Epoch 50: Cost = 0.6359\n",
      "Epoch 60: Cost = 0.6063\n",
      "Epoch 70: Cost = 0.5701\n",
      "Epoch 80: Cost = 0.5301\n",
      "Epoch 90: Cost = 0.4892\n",
      "Epoch 100: Cost = 0.4493\n",
      "Epoch 110: Cost = 0.4118\n",
      "Epoch 120: Cost = 0.3771\n",
      "Epoch 130: Cost = 0.3457\n",
      "Epoch 140: Cost = 0.3176\n",
      "Epoch 150: Cost = 0.2927\n",
      "Epoch 160: Cost = 0.2707\n",
      "Epoch 170: Cost = 0.2512\n",
      "Epoch 180: Cost = 0.2340\n",
      "Epoch 190: Cost = 0.2188\n",
      "Epoch 200: Cost = 0.2054\n",
      "Epoch 210: Cost = 0.1934\n",
      "Epoch 220: Cost = 0.1828\n",
      "Epoch 230: Cost = 0.1734\n",
      "Epoch 240: Cost = 0.1648\n",
      "Epoch 250: Cost = 0.1571\n",
      "Epoch 260: Cost = 0.1501\n",
      "Epoch 270: Cost = 0.1438\n",
      "Epoch 280: Cost = 0.1380\n",
      "Epoch 290: Cost = 0.1327\n",
      "Epoch 300: Cost = 0.1279\n",
      "Epoch 310: Cost = 0.1234\n",
      "Epoch 320: Cost = 0.1193\n",
      "Epoch 330: Cost = 0.1154\n",
      "Epoch 340: Cost = 0.1119\n",
      "Epoch 350: Cost = 0.1085\n",
      "Epoch 360: Cost = 0.1054\n",
      "Epoch 370: Cost = 0.1025\n",
      "Epoch 380: Cost = 0.0998\n",
      "Epoch 390: Cost = 0.0972\n",
      "Epoch 400: Cost = 0.0948\n",
      "Training MLP with RELU activation and 2 hidden layer(s)...\n",
      "Epoch 10: Cost = 0.6901\n",
      "Epoch 20: Cost = 0.6884\n",
      "Epoch 30: Cost = 0.6872\n",
      "Epoch 40: Cost = 0.6864\n",
      "Epoch 50: Cost = 0.6857\n",
      "Epoch 60: Cost = 0.6851\n",
      "Epoch 70: Cost = 0.6846\n",
      "Epoch 80: Cost = 0.6840\n",
      "Epoch 90: Cost = 0.6834\n",
      "Epoch 100: Cost = 0.6826\n",
      "Epoch 110: Cost = 0.6817\n",
      "Epoch 120: Cost = 0.6804\n",
      "Epoch 130: Cost = 0.6787\n",
      "Epoch 140: Cost = 0.6765\n",
      "Epoch 150: Cost = 0.6734\n",
      "Epoch 160: Cost = 0.6690\n",
      "Epoch 170: Cost = 0.6628\n",
      "Epoch 180: Cost = 0.6537\n",
      "Epoch 190: Cost = 0.6403\n",
      "Epoch 200: Cost = 0.6206\n",
      "Epoch 210: Cost = 0.5924\n",
      "Epoch 220: Cost = 0.5533\n",
      "Epoch 230: Cost = 0.5021\n",
      "Epoch 240: Cost = 0.4395\n",
      "Epoch 250: Cost = 0.3710\n",
      "Epoch 260: Cost = 0.3023\n",
      "Epoch 270: Cost = 0.2403\n",
      "Epoch 280: Cost = 0.1891\n",
      "Epoch 290: Cost = 0.1502\n",
      "Epoch 300: Cost = 0.1223\n",
      "Epoch 310: Cost = 0.1027\n",
      "Epoch 320: Cost = 0.0888\n",
      "Epoch 330: Cost = 0.0787\n",
      "Epoch 340: Cost = 0.0711\n",
      "Epoch 350: Cost = 0.0653\n",
      "Epoch 360: Cost = 0.0607\n",
      "Epoch 370: Cost = 0.0570\n",
      "Epoch 380: Cost = 0.0540\n",
      "Epoch 390: Cost = 0.0514\n",
      "Epoch 400: Cost = 0.0493\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for activation in [\"tanh\", \"relu\"]:\n",
    "    for layers in [[5], [5, 5]]:\n",
    "        print(f\"Training MLP with {activation.upper()} activation and {len(layers)} hidden layer(s)...\")\n",
    "        mlp = MLP(input_size=X.shape[1], hidden_layers=layers, learning_rate=0.1, activation=activation)\n",
    "        mlp.train(X_train, y_train, epochs=400)\n",
    "        y_pred = mlp.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        results.append((activation, len(layers), accuracy, report))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Comparison of Models:\n",
      "============================================================\n",
      "Activation: TANH, Hidden Layers: 1\n",
      "Accuracy: 0.9891\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       153\n",
      "           1       0.98      1.00      0.99       122\n",
      "\n",
      "    accuracy                           0.99       275\n",
      "   macro avg       0.99      0.99      0.99       275\n",
      "weighted avg       0.99      0.99      0.99       275\n",
      "\n",
      "------------------------------------------------------------\n",
      "Activation: TANH, Hidden Layers: 2\n",
      "Accuracy: 0.9927\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       153\n",
      "           1       0.98      1.00      0.99       122\n",
      "\n",
      "    accuracy                           0.99       275\n",
      "   macro avg       0.99      0.99      0.99       275\n",
      "weighted avg       0.99      0.99      0.99       275\n",
      "\n",
      "------------------------------------------------------------\n",
      "Activation: RELU, Hidden Layers: 1\n",
      "Accuracy: 0.9891\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       153\n",
      "           1       0.98      1.00      0.99       122\n",
      "\n",
      "    accuracy                           0.99       275\n",
      "   macro avg       0.99      0.99      0.99       275\n",
      "weighted avg       0.99      0.99      0.99       275\n",
      "\n",
      "------------------------------------------------------------\n",
      "Activation: RELU, Hidden Layers: 2\n",
      "Accuracy: 0.9927\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       153\n",
      "           1       0.98      1.00      0.99       122\n",
      "\n",
      "    accuracy                           0.99       275\n",
      "   macro avg       0.99      0.99      0.99       275\n",
      "weighted avg       0.99      0.99      0.99       275\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinal Comparison of Models:\")\n",
    "print(\"=\" * 60)\n",
    "for activation, num_layers, accuracy, report in results:\n",
    "    print(f\"Activation: {activation.upper()}, Hidden Layers: {num_layers}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
